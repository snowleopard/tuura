# Tuura

Tuura ([Kyrgyz](http://en.wikipedia.org/wiki/Kyrgyz_language): ту́урa, which means _correct_) is a set of open source software tools to automate collaborative testing of correctness and performance of algorithms. Testing is conducted in the form of Tuura tournaments (or _tuurnaments_) — algorithmic competitions, whose participants compete against each other in problem solving and testing skills.

This project has two primary goals: to provide a platform for organisation of online algorithmic contests, and to provide a way to collaboratively test and compare different solutions of an algorithmic problem.

## Rules

The rules are not defined completely yet and can be changed later; please see the [project forum](http://groups.google.com/group/tuuraproject) for current discussions. The following summarises the main idea behind the project.

### Writing a problem

Any participant can write a _problem_ for a tuurnament. A problem description consists of:

* a _problem statement_ specifying a particular algorithmic task which has to be solved;
* a set of _tests_, input files which have to be processed by a _solution_ (optional);
* a _test validator_, a program checking correctness of a test;
* a _reference solution_ to the problem, which is used to verify correctness of output files generated by all other solutions.

### Solving a problem

Any participant can write a _solution_ for a problem in any supported programming language. The solution is accepted as correct (_tuura_!) if it passes all currently available tests; if it fails even a single test, the solution is rejected (_tuura emes_, which means _incorrect_). A solution passes a test if it generates a correct output file within a time limit specified in the problem statement.

Accepted solutions are ranked according to the total runtime spent on solving all tests.

### Testing solutions

Any participant can add a test to the shared pool of tests available for a problem. The test is validated by the test validator provided by the problem's author. All previously accepted solutions are run on the new test and either remain accepted (their total runtimes are then increased accordingly) or become rejected.

Tests are ranked according to the number of solutions they fail.
